{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We often use GPUs to train and deploy neural networks, because it offers significant more computation power compared to CPUs. In this tutorial we will introduce how to use GPUs with MXNet.\n",
    "\n",
    "First, make sure you have at least one Nvidia GPU in your machine and CUDA\n",
    "properly installed. Other GPUs such as AMD and Intel GPUs are not supported\n",
    "yet. Then be sure you have\n",
    "[installed the GPU-enabled version of MXNet](mxnet_packages.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# If you pip installed the plain `mxnet` before, uncomment the\n",
    "# following two lines to install the GPU version. You may need to\n",
    "# replace `cu91` according to your CUDA version.\n",
    "#\n",
    "# !pip uninstall mxnet\n",
    "# !pip install mxnet-cu91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd, gpu, gluon, autograd\n",
    "\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import datasets, transforms\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Allocate data to a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You may notice that MXNet's NDArray is very similar to Numpy. One major difference is NDArray has a `context` attribute that specifies which device this array is on. By default, it is `cpu()`. Now we will change it to the first GPU. You can use `gpu()` or `gpu(0)` to indicate the first GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.  1.  1.  1.]\n",
       " [ 1.  1.  1.  1.]\n",
       " [ 1.  1.  1.  1.]]\n",
       "<NDArray 3x4 @gpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.ones((3,4), ctx=gpu())\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For a CPU, MXNet will allocate data on main memory, and try to use all CPU cores as possible, even if there is more than one CPU socket. While if there are multiple GPUs, MXNet needs to specify which GPUs the NDArray will be allocated.\n",
    "\n",
    "Let's assume there is a least one more GPU. We can create another NDArray and assign it there. (If you only have one GPU, then you will see an error). Here we copy `x` to the second GPU, `gpu(1)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.  1.  1.  1.]\n",
       " [ 1.  1.  1.  1.]\n",
       " [ 1.  1.  1.  1.]]\n",
       "<NDArray 3x4 @gpu(1)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.copyto(gpu(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Several operators such as `print`, `asnumpy` and `asscalar`, will implicitly move data to main memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "MXNet needs users to explicitly move data between devices. But several operators such as `print`, `asnumpy` and `asscalar`, will implicitly move data to main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run an operation on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To perform an operation on a particular GPU, we only need to guarantee that the inputs of this operation are already on that GPU. The output will be allocated on the same GPU as well. Almost all operators in the `nd` module support running on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "21"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.66865087  1.17409194  1.38500249  1.24678314]\n",
       " [ 1.35134339  1.84042978  1.63699174  1.12846994]\n",
       " [ 1.17249882  1.93682063  1.59183455  1.94970965]]\n",
       "<NDArray 3x4 @gpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = nd.random.uniform(shape=(3,4), ctx=gpu(0))\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Remember that if the inputs are not on the same GPU, you will see an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run a neural network on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Similarly, to run a neural network on a GPU, we only need to copy/move the input data and parameters to the GPU. Let's reuse the previously defined LeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Conv2D(channels=6, kernel_size=5, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(120, activation=\"relu\"),\n",
    "        nn.Dense(84, activation=\"relu\"),\n",
    "        nn.Dense(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "And then load the saved parameters into GPU 0 directly, or use `net.collect_params().reset_ctx` to change the device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net.load_parameters('net.params', ctx=gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "input data on GPU 0. forward pass will run on GPU 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "22"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.24491477 -0.92440087  0.92860025 -0.20226827 -0.01408684 -1.01782537\n",
       "   1.43946671 -1.03133845  0.57674176 -1.57243443]]\n",
       "<NDArray 1x10 @gpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.random.uniform(shape=(1,1,28,28), ctx=gpu(0))\n",
    "\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, we show how to use multiple GPUs to jointly train a neural network through data parallelism. Let's assume there are *n* GPUs. We split each data batch into *n* parts, and then each GPU will run the forward and backward passes using one part of the data.\n",
    "\n",
    "Let's first copy the data definitions and the transform function from the [previous tutorial](predict.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.13, 0.31)])\n",
    "\n",
    "train_data = gluon.data.DataLoader(\n",
    "    datasets.FashionMNIST(train=True).transform_first(transformer),\n",
    "    batch_size, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The training loop is quite similar to what we introduced before. The major differences are highlighted in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Diff 1: Use two GPUs for training.\n",
    "devices = [gpu(0), gpu(1)]\n",
    "\n",
    "# Diff 2: reinitialize the parameters and place them on multiple GPUs\n",
    "net.initialize(force_reinit=True, ctx=devices)\n",
    "\n",
    "# Loss and trainer are the same as before, lr scaled\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        'sgd', {'learning_rate': 0.1*len(devices)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 4.522, Perf 34818.1 img/sec\n",
      "Epoch 1: Loss: 2.069, Perf 42469.4 img/sec\n",
      "Epoch 2: Loss: 1.193, Perf 42641.4 img/sec\n",
      "Epoch 3: Loss: 1.054, Perf 40905.3 img/sec\n",
      "Epoch 4: Loss: 0.984, Perf 40661.8 img/sec\n",
      "Epoch 5: Loss: 0.922, Perf 41732.4 img/sec\n",
      "Epoch 6: Loss: 0.873, Perf 41070.7 img/sec\n",
      "Epoch 7: Loss: 0.839, Perf 40662.5 img/sec\n",
      "Epoch 8: Loss: 0.801, Perf 42109.3 img/sec\n",
      "Epoch 9: Loss: 0.777, Perf 38447.2 img/sec\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss = 0\n",
    "    tic = time()\n",
    "    for data, label in train_data:\n",
    "        # Diff 3: split batch and load into corresponding devices\n",
    "        data_list = gluon.utils.split_and_load(data, devices)\n",
    "        label_list = gluon.utils.split_and_load(label, devices)\n",
    "        # Diff 4: run forward and backward on each devices.\n",
    "        with autograd.record():\n",
    "            losses = [softmax_cross_entropy(net(X), y)\n",
    "                      for X, y in zip(data_list, label_list)]\n",
    "        for l in losses:\n",
    "            l.backward()\n",
    "        trainer.step(batch_size)\n",
    "        # Diff 5: sum losses across each devices\n",
    "        for i, l in enumerate(losses):\n",
    "            train_loss += l.mean().asscalar()\n",
    "            \n",
    "    print(\"Epoch %d: Loss: %.3f, Perf %.1f img/sec\" % (\n",
    "        epoch, train_loss/len(train_data), \n",
    "        len(train_data)*batch_size/(time()-tic)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
